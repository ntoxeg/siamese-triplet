{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image as PImage\n",
    "from IPython.display import Image as NbImage, display\n",
    "\n",
    "from fastai.vision import *\n",
    "from torchvision import transforms\n",
    "\n",
    "from data import ImageDataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(learner:Learner, img:Image):\n",
    "    return learner.predict(img)[1]\n",
    "\n",
    "def _embed_imgset(learner:Learner, imgset):\n",
    "    return [embed(learner, img) for img in imgset]\n",
    "\n",
    "def _find_nneighbours(learner:Learner, img:Image, embedlist, n=5):\n",
    "    imgembed = embed(learner, img)\n",
    "    scores = [torch.dot(imgembed-emb, imgembed-emb) for emb in embedlist]\n",
    "    results = sorted(zip(scores, range(len(embedlist))))\n",
    "    return list(zip(*results[:n]))[1]\n",
    "\n",
    "def find_nneighbours(learner:Learner, img:Image, imgset, n=5):\n",
    "    embedlist = _embed_dataset(learner, imgset)\n",
    "    imgembed = embed(learner, img).cpu()\n",
    "    scores = [F.cosine_similarity(imgembed, emb) for emb in embedlist]\n",
    "    results = sorted(zip(scores, range(len(imgset))), reverse=True)\n",
    "    return list(zip(*results[:n]))[1]\n",
    "\n",
    "def show_nneighbours(learner:Learner, img:Image, imgset, n=5, embedlist=None):\n",
    "    if embedlist is None:\n",
    "        embedlist = _embed_dataset(learner, imgset)\n",
    "    idxs = _find_nneighbours(learner, img, embedlist, n)\n",
    "    images = [NbImage(filename=datum[0]) for datum in [imgset[i] for i in idxs]]\n",
    "    display(*images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='data'\n",
    "data_split='val'\n",
    "classes = ('person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush')\n",
    "n_epochs = 1\n",
    "emsize = 32\n",
    "batch_size = 2\n",
    "margin = 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_epochs = 100\n",
    "batch_size = 8\n",
    "emsize = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    Image,\n",
    "    partial(crop_pad, size=512, padding_mode=\"zeros\")\n",
    "])\n",
    "dataset = ImageDataset(\n",
    "    f\"{data_dir}/classy_coconut/val\",\n",
    "    classes,\n",
    "    tfms=tfms\n",
    ")\n",
    "n_classes = len(classes)\n",
    "\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = load_learner(\"siamese\", \"export.pkl\").to_fp32()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedlist = _embed_dataset(learner, dataset)\n",
    "# np.save(\"classy_coconut_embeddings_val.npz\", torch.stack(embedlist).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tfms(PImage.open(\"images/000000008021-head.jpg\"))\n",
    "show_nneighbours(learner, img, dataset, embedlist=embedlist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
