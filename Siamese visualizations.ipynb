{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "sx9e_pXlCuti",
    "papermill": {
     "duration": 0.025469,
     "end_time": "2019-09-24T18:13:15.750201",
     "exception": false,
     "start_time": "2019-09-24T18:13:15.724732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false,
    "papermill": {
     "duration": 0.021887,
     "end_time": "2019-09-24T18:13:15.779714",
     "exception": false,
     "start_time": "2019-09-24T18:13:15.757827",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "data_dir='data'\n",
    "data_split='val'\n",
    "classes = ('person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush')\n",
    "n_epochs = 1\n",
    "emsize = 32\n",
    "batch_size = 2\n",
    "margin = 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.016532,
     "end_time": "2019-09-24T18:13:15.804530",
     "exception": false,
     "start_time": "2019-09-24T18:13:15.787998",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_epochs = 50\n",
    "batch_size = 8\n",
    "emsize = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false,
    "papermill": {
     "duration": 0.970821,
     "end_time": "2019-09-24T18:13:16.781860",
     "exception": false,
     "start_time": "2019-09-24T18:13:15.811039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "from PIL import Image as PImage\n",
    "import os\n",
    "from datasets import ImageDataset, pad_to_size\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from ivis import Ivis\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from fastai.vision import *\n",
    "from networks import siamese_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false,
    "papermill": {
     "duration": 0.219969,
     "end_time": "2019-09-24T18:13:17.010556",
     "exception": false,
     "start_time": "2019-09-24T18:13:16.790587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "tfms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(*imagenet_stats),\n",
    "    lambda x: pad_to_size(x, (512, 512))\n",
    "])\n",
    "train_dataset = ImageDataset(\n",
    "    f\"{data_dir}/classy_coconut/train\",\n",
    "    classes,\n",
    "    tfms=tfms\n",
    ")\n",
    "val_dataset = ImageDataset(\n",
    "    f\"{data_dir}/classy_coconut/val\",\n",
    "    classes,\n",
    "    tfms=tfms\n",
    ")\n",
    "n_classes = len(classes)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false,
    "papermill": {
     "duration": 1.278209,
     "end_time": "2019-09-24T18:13:18.297700",
     "exception": false,
     "start_time": "2019-09-24T18:13:17.019491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import SiameseImage\n",
    "\n",
    "siamese_train_dataset = SiameseImage(train_dataset, True) # Returns pairs of images and target same/different\n",
    "siamese_val_dataset = SiameseImage(val_dataset, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false,
    "papermill": {
     "duration": 1.033832,
     "end_time": "2019-09-24T18:13:19.339520",
     "exception": false,
     "start_time": "2019-09-24T18:13:18.305688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import SiameseImageList\n",
    "lls = SiameseImageList.from_datasets(siamese_train_dataset, siamese_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false,
    "papermill": {
     "duration": 0.124846,
     "end_time": "2019-09-24T18:13:19.473426",
     "exception": false,
     "start_time": "2019-09-24T18:13:19.348580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dbunch = (lls\n",
    "          .transform(get_transforms())\n",
    "          .databunch(\"siamese\", bs=batch_size, device=\"cuda:0\", num_workers=8)\n",
    "          .normalize(imagenet_stats)\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false,
    "papermill": {
     "duration": 2.09525,
     "end_time": "2019-09-24T18:13:21.574669",
     "exception": false,
     "start_time": "2019-09-24T18:13:19.479419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "learner = siamese_learner(dbunch, models.resnet50, emsize, margin)\n",
    "learner.load(\"pair_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from trainer import fit\n",
    "import numpy as np\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors = ['#ACB556',\n",
    " '#6B0519',\n",
    " '#317ADD',\n",
    " '#E89478',\n",
    " '#E2C6BB',\n",
    " '#11A545',\n",
    " '#04DA93',\n",
    " '#255C23',\n",
    " '#E1E690',\n",
    " '#E46065',\n",
    " '#C6F8A5',\n",
    " '#7300E8',\n",
    " '#903A28',\n",
    " '#89165F',\n",
    " '#24B131',\n",
    " '#D529F8',\n",
    " '#B406FA',\n",
    " '#1BFC03',\n",
    " '#C88197',\n",
    " '#E049C6',\n",
    " '#6CA73E',\n",
    " '#FC2E30',\n",
    " '#50A394',\n",
    " '#8EA6A3',\n",
    " '#E42044',\n",
    " '#EA78E5',\n",
    " '#8F6F2A',\n",
    " '#6F5BC9',\n",
    " '#E28F92',\n",
    " '#356097',\n",
    " '#BDA255',\n",
    " '#A68CDD',\n",
    " '#1A8127',\n",
    " '#E35BB1',\n",
    " '#C303BF',\n",
    " '#A0E7EE',\n",
    " '#2A1574',\n",
    " '#70D458',\n",
    " '#DBFB14',\n",
    " '#ED081D',\n",
    " '#51EECB',\n",
    " '#442A27',\n",
    " '#C84D7C',\n",
    " '#C9118E',\n",
    " '#CA9152',\n",
    " '#55287F',\n",
    " '#86DC17',\n",
    " '#9DB452',\n",
    " '#554D45',\n",
    " '#4D3C0F',\n",
    " '#F760EA',\n",
    " '#655EB4',\n",
    " '#4AC7C6',\n",
    " '#E8EE1E',\n",
    " '#9F8EC0',\n",
    " '#BF9D25',\n",
    " '#7DC9E2',\n",
    " '#241DD7',\n",
    " '#6E3114',\n",
    " '#3AE5E7',\n",
    " '#AC18DC',\n",
    " '#EEA33D',\n",
    " '#179935',\n",
    " '#F05348',\n",
    " '#387F47',\n",
    " '#78C6A6',\n",
    " '#5308BD',\n",
    " '#51A2C3',\n",
    " '#FB8106',\n",
    " '#4F675C',\n",
    " '#12BBBC',\n",
    " '#716F88',\n",
    " '#33E6BF',\n",
    " '#0B46F3',\n",
    " '#500A7C',\n",
    " '#5F4F1A',\n",
    " '#25E5F8',\n",
    " '#8C1021',\n",
    " '#7D6EF2',\n",
    " '#F3D3A0']\n",
    "\n",
    "def plot_embeddings(embeddings, targets, classes, xlim=None, ylim=None, figsize=(30, 30)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(n_classes):\n",
    "        inds = np.where(targets==i)[0]\n",
    "        plt.scatter(embeddings[inds,0], embeddings[inds,1], alpha=0.5, color=colors[i])\n",
    "    if xlim:\n",
    "        plt.xlim(xlim[0], xlim[1])\n",
    "    if ylim:\n",
    "        plt.ylim(ylim[0], ylim[1])\n",
    "    plt.legend(classes)\n",
    "\n",
    "def extract_embeddings(dataloader, model):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        embeddings = np.zeros((len(dataloader.dataset), emsize))\n",
    "        labels = np.zeros(len(dataloader.dataset))\n",
    "        k = 0\n",
    "        for images, target in dataloader:\n",
    "            if cuda:\n",
    "                images = images.cuda()\n",
    "            embeddings[k:k+len(images)] = model.get_embedding(images).data.cpu().numpy()\n",
    "            labels[k:k+len(images)] = target.numpy()\n",
    "            k += len(images)\n",
    "    return embeddings, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_embeddings_cl, train_labels_cl = extract_embeddings(train_loader, model)\n",
    "# scaler = StandardScaler().fit(train_embeddings_cl)\n",
    "# train_embeddings_tsne = TSNE(n_components=2, random_state=1337).fit_transform(scaler.transform(train_embeddings_cl))\n",
    "# plot_embeddings(train_embeddings_tsne, train_labels_cl)\n",
    "model = learner.model\n",
    "val_embeddings_cl, val_labels_cl = extract_embeddings(val_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler().fit(val_embeddings_cl)\n",
    "# val_embeddings_tsne = TSNE(n_components=2, random_state=1337).fit_transform(scaler.transform(val_embeddings_cl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ivis_ = Ivis(embedding_dims=2, k=15, model=\"maaten\")\n",
    "val_embeddings_ivis2 = ivis_.fit_transform(MinMaxScaler().fit_transform(val_embeddings_cl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ivis_ = Ivis(embedding_dims=2, k=15, model=\"hinton\")\n",
    "# val_embeddings_ivis3 = ivis_.fit_transform(MinMaxScaler().fit_transform(val_embeddings_cl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_embeddings(val_embeddings_tsne, val_labels_cl, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embeddings(val_embeddings_ivis2, val_labels_cl, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_embeddings(val_embeddings_ivis3, val_labels_cl, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widget_classes = ['back_button', 'browser_address', 'browser_bookmark', 'browser_tab', 'button', 'checkbox', 'close_button', 'menu', 'minimize_button', 'next_button', 'slider', 'text_field', 'text_object']\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728',\n",
    "              '#9467bd', '#8c564b', '#e377c2', '#7f7f7f',\n",
    "              '#bcbd22', '#17becf', '#800080', '#808000', '#F08080']\n",
    "widget_dataset = ImageDataset('/home/adrian/data/Widgets/train',\n",
    "                            widget_classes,\n",
    "    tfms=tfms)\n",
    "n_classes = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widget_loader = torch.utils.data.DataLoader(widget_dataset, batch_size=batch_size, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widget_embeddings_cl, widget_labels_cl = extract_embeddings(widget_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ivis_ = Ivis(embedding_dims=2, k=5, model=\"maaten\", batch_size=4)\n",
    "widget_embeddings_ivis = ivis_.fit_transform(MinMaxScaler().fit_transform(widget_embeddings_cl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embeddings(widget_embeddings_ivis, widget_labels_cl, widget_classes, figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_idxs = np.random.choice(np.arange(widget_embeddings_cl.shape[0]), 30, replace=False)\n",
    "plot_embeddings(widget_embeddings_ivis[plot_idxs], widget_labels_cl[plot_idxs], widget_classes, figsize=(10, 10))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "default_view": {},
   "name": "Experiments_MNIST.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "nteract": {
   "version": "0.14.5"
  },
  "papermill": {
   "duration": 29512.950271,
   "end_time": "2019-09-25T02:25:08.140387",
   "environment_variables": {},
   "exception": null,
   "input_path": "Siamese ResNet50.ipynb",
   "output_path": "Siamese ResNet50 output.ipynb",
   "parameters": {
    "batch_size": 8,
    "n_epochs": 20
   },
   "start_time": "2019-09-24T18:13:15.190116",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
